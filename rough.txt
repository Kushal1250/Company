# üéØ COMPLETE PROJECT SETUP GUIDE - VoiceMind Meeting Intelligence System
I've analyzed all the provided information and created a **comprehensive, step-by-step implementation guide** with proper folder structure, MySQL database, VS Code setup, and complete workflow.
---
## üìÅ PROJECT FOLDER STRUCTURE
### **Tree Structure**
```
VoiceMind-Meeting-System/
‚îÇ
‚îú‚îÄ‚îÄ hardware/
‚îÇ ‚îú‚îÄ‚îÄ esp32_audio_recorder/
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ esp32_audio_recorder.ino # ESP32 Arduino code
‚îÇ ‚îî‚îÄ‚îÄ circuit_diagrams/
‚îÇ ‚îî‚îÄ‚îÄ breadboard_connections.txt # Pin connections
‚îÇ
‚îú‚îÄ‚îÄ backend/
‚îÇ ‚îú‚îÄ‚îÄ server.py # Main FastAPI server
‚îÇ ‚îú‚îÄ‚îÄ database.py # Database connection & models
‚îÇ ‚îú‚îÄ‚îÄ audio_processor.py # Audio processing logic
‚îÇ ‚îú‚îÄ‚îÄ transcription_service.py # Whisper integration
‚îÇ ‚îú‚îÄ‚îÄ qa_service.py # AI Q&A with LLM
‚îÇ ‚îú‚îÄ‚îÄ config.py # Configuration settings
‚îÇ ‚îî‚îÄ‚îÄ requirements.txt # Python dependencies
‚îÇ
‚îú‚îÄ‚îÄ client/
‚îÇ ‚îî‚îÄ‚îÄ meeting_client.py # CLI client interface
‚îÇ
‚îú‚îÄ‚îÄ database/
‚îÇ ‚îú‚îÄ‚îÄ schema.sql # MySQL database schema
‚îÇ ‚îî‚îÄ‚îÄ init_db.py # Database initialization script
‚îÇ
‚îú‚îÄ‚îÄ uploads/
‚îÇ ‚îî‚îÄ‚îÄ audio_chunks/ # Temporary audio storage
‚îÇ
‚îú‚îÄ‚îÄ logs/
‚îÇ ‚îî‚îÄ‚îÄ app.log # Application logs
‚îÇ
‚îú‚îÄ‚îÄ .env # Environment variables
‚îú‚îÄ‚îÄ README.md # Project documentation
‚îî‚îÄ‚îÄ setup.sh # Automated setup script
```
---
## üî® STEP 1: CREATE PROJECT STRUCTURE
### **Commands to Create Folders and Files**
Open **VS Code Terminal** (Ctrl+` or View > Terminal) and run:
```bash
# Create main project directory
mkdir VoiceMind-Meeting-System
cd VoiceMind-Meeting-System
# Create folder structure
mkdir -p hardware/esp32_audio_recorder
mkdir -p hardware/circuit_diagrams
mkdir -p backend
mkdir -p client
mkdir -p database
mkdir -p uploads/audio_chunks
mkdir -p logs
# Create files
touch hardware/esp32_audio_recorder/esp32_audio_recorder.ino
touch hardware/circuit_diagrams/breadboard_connections.txt
touch backend/server.py
touch backend/database.py
touch backend/audio_processor.py
touch backend/transcription_service.py
touch backend/qa_service.py
touch backend/config.py
touch backend/requirements.txt
touch client/meeting_client.py
touch database/schema.sql
touch database/init_db.py
touch .env
touch README.md
touch setup.sh
# Make setup script executable
chmod +x setup.sh
```
---
## üìä STEP 2: BREADBOARD PIN CONNECTIONS
### **File: `hardware/circuit_diagrams/breadboard_connections.txt`**
```
================================================================================
ESP32 + INMP441 I2S MEMS MICROPHONE - BREADBOARD CONNECTION GUIDE
================================================================================
COMPONENT PLACEMENT ON BREADBOARD:
-----------------------------------
ROW 1-5: ESP32 DevKit (30-pin)
ROW 10-13: INMP441 MEMS Microphone Module
PIN-TO-PIN CONNECTIONS:
-----------------------
ESP32 Side ‚Üí INMP441 Side ‚Üí Breadboard Position
----------------------------------------------------------------------------
3.3V (Pin 1) ‚Üí VDD ‚Üí Power Rail (+)
GND (Pin 2) ‚Üí GND ‚Üí Power Rail (-)
GPIO32 (Pin 8) ‚Üí SD (Serial Data) ‚Üí Row 11, Column C
GPIO25 (Pin 10) ‚Üí WS (Word Select) ‚Üí Row 12, Column C
GPIO33 (Pin 13) ‚Üí SCK (Serial Clock) ‚Üí Row 13, Column C
GND (Pin 38) ‚Üí L/R (Left/Right) ‚Üí Power Rail (-)
ADDITIONAL COMPONENTS:
---------------------
‚Ä¢ LED (Status Indicator):
  - GPIO2 ‚Üí 220Œ© Resistor ‚Üí LED Anode ‚Üí LED Cathode ‚Üí GND
‚Ä¢ Push Button (Start/Stop Recording):
  - GPIO4 ‚Üí Button Pin 1
  - Button Pin 2 ‚Üí GND
  - GPIO4 ‚Üí 10kŒ© Pull-up Resistor ‚Üí 3.3V
POWER SUPPLY:
-------------
‚Ä¢ Connect ESP32 to USB power (5V, 1A minimum)
‚Ä¢ OR use external 5V power supply via VIN pin
VISUAL ASCII DIAGRAM:
--------------------
        [USB Power]
             |
    +--------+--------+
    | |
    | ESP32 DevKit |
    | |
    | 3.3V‚óè ‚óèGND |‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    | IO32‚óè | ‚îÇ
    | IO25‚óè | ‚îÇ
    | IO33‚óè | ‚îÇ
    | GND‚óè ‚óèGPIO2 |‚îÄ‚îÄ‚îê ‚îÇ
    | ‚óèGPIO4 | ‚îÇ ‚îÇ
    +--------+--------+ ‚îÇ ‚îÇ
             | ‚îÇ ‚îÇ
         [WiFi] ‚îÇ ‚îÇ
                         ‚îÇ ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ INMP441 ‚îÇ
                    ‚îÇ MEMS MIC ‚îÇ
                    ‚îÇ ‚îÇ
                    ‚îÇ VDD GND ‚îÇ
                    ‚îÇ SD WS SCK ‚îÇ
                    ‚îÇ L/R ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
TESTING CHECKLIST:
-----------------
‚òê Verify 3.3V on VDD pin (use multimeter)
‚òê Check continuity on all connections
‚òê Ensure L/R is connected to GND (for left channel)
‚òê Test WiFi connectivity before audio streaming
‚òê Verify I2S pins are correctly mapped in code
================================================================================
```
---
## üóÑÔ∏è STEP 3: MYSQL DATABASE SCHEMA
### **File: `database/schema.sql`**
```sql
-- ============================================================================
-- VoiceMind Meeting Intelligence System - MySQL Database Schema
-- ============================================================================
-- Drop existing database if exists (WARNING: This deletes all data!)
DROP DATABASE IF EXISTS voicemind_db;
-- Create database
CREATE DATABASE voicemind_db CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
USE voicemind_db;
-- ============================================================================
-- TABLE: meetings
-- Stores meeting metadata and full transcripts
-- ============================================================================
CREATE TABLE meetings (
    id INT AUTO_INCREMENT PRIMARY KEY,
    meeting_id VARCHAR(100) UNIQUE NOT NULL,
    title VARCHAR(255),
    start_time DATETIME DEFAULT CURRENT_TIMESTAMP,
    end_time DATETIME NULL,
    status ENUM('recording', 'processing', 'completed', 'failed') DEFAULT 'recording',
    language VARCHAR(10) DEFAULT 'auto',
    full_transcript TEXT,
    summary TEXT,
    agenda TEXT,
    total_chunks INT DEFAULT 0,
    total_duration FLOAT DEFAULT 0.0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    INDEX idx_meeting_id (meeting_id),
    INDEX idx_status (status),
    INDEX idx_start_time (start_time)
) ENGINE=InnoDB;
-- ============================================================================
-- TABLE: audio_chunks
-- Stores individual audio segments with transcriptions
-- ============================================================================
CREATE TABLE audio_chunks (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    meeting_id VARCHAR(100) NOT NULL,
    chunk_number INT NOT NULL,
    chunk_timestamp BIGINT NOT NULL,
    audio_data LONGBLOB,
    audio_file_path VARCHAR(500),
    sample_rate INT DEFAULT 16000,
    duration FLOAT,
    transcript_segment TEXT,
    language_detected VARCHAR(10),
    confidence_score FLOAT,
    speaker_id VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (meeting_id) REFERENCES meetings(meeting_id) ON DELETE CASCADE,
    UNIQUE KEY unique_chunk (meeting_id, chunk_number),
    INDEX idx_meeting_chunks (meeting_id, chunk_number),
    INDEX idx_timestamp (chunk_timestamp)
) ENGINE=InnoDB;
-- ============================================================================
-- TABLE: qa_history
-- Stores question-answer interactions for each meeting
-- ============================================================================
CREATE TABLE qa_history (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    meeting_id VARCHAR(100) NOT NULL,
    question TEXT NOT NULL,
    answer TEXT NOT NULL,
    context_used TEXT,
    model_used VARCHAR(50),
    response_time FLOAT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (meeting_id) REFERENCES meetings(meeting_id) ON DELETE CASCADE,
    INDEX idx_meeting_qa (meeting_id),
    INDEX idx_created_at (created_at)
) ENGINE=InnoDB;
-- ============================================================================
-- TABLE: system_logs
-- Stores system events and errors
-- ============================================================================
CREATE TABLE system_logs (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    log_level ENUM('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL') DEFAULT 'INFO',
    meeting_id VARCHAR(100),
    message TEXT NOT NULL,
    stack_trace TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_log_level (log_level),
    INDEX idx_meeting_logs (meeting_id),
    INDEX idx_created_at (created_at)
) ENGINE=InnoDB;
-- ============================================================================
-- VIEWS: Useful aggregated views
-- ============================================================================
-- View: Meeting statistics
CREATE VIEW meeting_stats AS
SELECT
    m.meeting_id,
    m.title,
    m.status,
    m.start_time,
    m.end_time,
    TIMESTAMPDIFF(MINUTE, m.start_time, m.end_time) AS duration_minutes,
    m.total_chunks,
    COUNT(ac.id) AS chunks_stored,
    SUM(ac.duration) AS total_audio_duration,
    COUNT(qa.id) AS questions_asked
FROM meetings m
LEFT JOIN audio_chunks ac ON m.meeting_id = ac.meeting_id
LEFT JOIN qa_history qa ON m.meeting_id = qa.meeting_id
GROUP BY m.meeting_id;
-- ============================================================================
-- STORED PROCEDURES
-- ============================================================================
DELIMITER //
-- Procedure: Get meeting summary with statistics
CREATE PROCEDURE GetMeetingSummary(IN p_meeting_id VARCHAR(100))
BEGIN
    SELECT
        m.*,
        COUNT(ac.id) AS total_chunks_stored,
        SUM(ac.duration) AS total_duration_seconds,
        COUNT(DISTINCT ac.language_detected) AS languages_detected,
        COUNT(qa.id) AS total_questions_asked
    FROM meetings m
    LEFT JOIN audio_chunks ac ON m.meeting_id = ac.meeting_id
    LEFT JOIN qa_history qa ON m.meeting_id = qa.meeting_id
    WHERE m.meeting_id = p_meeting_id
    GROUP BY m.id;
END //
-- Procedure: Clean up old meetings (older than 30 days)
CREATE PROCEDURE CleanupOldMeetings()
BEGIN
    DELETE FROM meetings
    WHERE created_at < DATE_SUB(NOW(), INTERVAL 30 DAY)
    AND status = 'completed';
END //
DELIMITER ;
-- ============================================================================
-- INITIAL DATA (Optional test data)
-- ============================================================================
-- Insert sample meeting for testing
INSERT INTO meetings (meeting_id, title, status)
VALUES ('test_meeting_001', 'Test Meeting', 'recording');
-- ============================================================================
-- GRANT PERMISSIONS (Adjust username/password as needed)
-- ============================================================================
-- Create application user (run this separately after database creation)
-- CREATE USER 'voicemind_user'@'localhost' IDENTIFIED BY 'your_secure_password';
-- GRANT ALL PRIVILEGES ON voicemind_db.* TO 'voicemind_user'@'localhost';
-- FLUSH PRIVILEGES;
-- ============================================================================
-- END OF SCHEMA
-- ============================================================================
```
---
## üêç STEP 4: BACKEND IMPLEMENTATION
### **File: `backend/config.py`**
```python
"""
Configuration settings for VoiceMind Meeting System
"""
import os
from dotenv import load_dotenv
load_dotenv()
# Server Configuration
SERVER_HOST = os.getenv("SERVER_HOST", "0.0.0.0")
SERVER_PORT = int(os.getenv("SERVER_PORT", 8000))
# Database Configuration (MySQL)
DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = int(os.getenv("DB_PORT", 3306))
DB_USER = os.getenv("DB_USER", "root")
DB_PASSWORD = os.getenv("DB_PASSWORD", "")
DB_NAME = os.getenv("DB_NAME", "voicemind_db")
# OpenAI API Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
# Audio Processing Configuration
SAMPLE_RATE = 16000
CHUNK_DURATION_MS = 10000 # 10 seconds
UPLOAD_FOLDER = "uploads/audio_chunks"
LOG_FOLDER = "logs"
# Whisper Model Configuration
WHISPER_MODEL = "base" # Options: tiny, base, small, medium, large
# LLM Configuration
LLM_MODEL = "gpt-3.5-turbo"
LLM_TEMPERATURE = 0.7
MAX_TOKENS = 500
# Create necessary directories
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(LOG_FOLDER, exist_ok=True)
```
---
### **File: `backend/database.py`**
```python
"""
Database connection and ORM models using MySQL
"""
import mysql.connector
from mysql.connector import Error
from contextlib import contextmanager
import logging
from config import DB_HOST, DB_PORT, DB_USER, DB_PASSWORD, DB_NAME
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
class Database:
    """MySQL database connection manager"""
   
    def __init__(self):
        self.connection = None
   
    def connect(self):
        """Establish database connection"""
        try:
            self.connection = mysql.connector.connect(
                host=DB_HOST,
                port=DB_PORT,
                user=DB_USER,
                password=DB_PASSWORD,
                database=DB_NAME,
                charset='utf8mb4',
                use_unicode=True
            )
            if self.connection.is_connected():
                logger.info("Successfully connected to MySQL database")
                return self.connection
        except Error as e:
            logger.error(f"Error connecting to MySQL: {e}")
            raise
   
    def close(self):
        """Close database connection"""
        if self.connection and self.connection.is_connected():
            self.connection.close()
            logger.info("MySQL connection closed")
   
    @contextmanager
    def get_cursor(self, dictionary=True):
        """Context manager for database cursor"""
        cursor = self.connection.cursor(dictionary=dictionary)
        try:
            yield cursor
            self.connection.commit()
        except Exception as e:
            self.connection.rollback()
            logger.error(f"Database error: {e}")
            raise
        finally:
            cursor.close()
# Global database instance
db = Database()
def init_database():
    """Initialize database connection"""
    db.connect()
def close_database():
    """Close database connection"""
    db.close()
# ============================================================================
# Database Helper Functions
# ============================================================================
def create_meeting(meeting_id: str, title: str = None, language: str = "auto"):
    """Create a new meeting record"""
    with db.get_cursor() as cursor:
        sql = """
            INSERT INTO meetings (meeting_id, title, status, language)
            VALUES (%s, %s, %s, %s)
        """
        cursor.execute(sql, (meeting_id, title, 'recording', language))
        return cursor.lastrowid
def get_meeting(meeting_id: str):
    """Retrieve meeting by ID"""
    with db.get_cursor() as cursor:
        sql = "SELECT * FROM meetings WHERE meeting_id = %s"
        cursor.execute(sql, (meeting_id,))
        return cursor.fetchone()
def update_meeting_status(meeting_id: str, status: str, transcript: str = None, summary: str = None):
    """Update meeting status and transcript"""
    with db.get_cursor() as cursor:
        sql = """
            UPDATE meetings
            SET status = %s, full_transcript = %s, summary = %s, end_time = NOW()
            WHERE meeting_id = %s
        """
        cursor.execute(sql, (status, transcript, summary, meeting_id))
def save_audio_chunk(meeting_id: str, chunk_number: int, chunk_timestamp: int,
                     audio_data: bytes, sample_rate: int, transcript: str = None):
    """Save audio chunk to database"""
    with db.get_cursor() as cursor:
        sql = """
            INSERT INTO audio_chunks
            (meeting_id, chunk_number, chunk_timestamp, audio_data, sample_rate, transcript_segment)
            VALUES (%s, %s, %s, %s, %s, %s)
            ON DUPLICATE KEY UPDATE
            audio_data = VALUES(audio_data),
            transcript_segment = VALUES(transcript_segment)
        """
        cursor.execute(sql, (meeting_id, chunk_number, chunk_timestamp, audio_data, sample_rate, transcript))
       
        # Update total chunks count
        sql_update = """
            UPDATE meetings
            SET total_chunks = (SELECT COUNT(*) FROM audio_chunks WHERE meeting_id = %s)
            WHERE meeting_id = %s
        """
        cursor.execute(sql_update, (meeting_id, meeting_id))
def get_all_chunks(meeting_id: str):
    """Retrieve all audio chunks for a meeting"""
    with db.get_cursor() as cursor:
        sql = """
            SELECT chunk_number, transcript_segment, duration, chunk_timestamp
            FROM audio_chunks
            WHERE meeting_id = %s
            ORDER BY chunk_number
        """
        cursor.execute(sql, (meeting_id,))
        return cursor.fetchall()
def save_qa_interaction(meeting_id: str, question: str, answer: str, model_used: str, response_time: float):
    """Save Q&A interaction"""
    with db.get_cursor() as cursor:
        sql = """
            INSERT INTO qa_history (meeting_id, question, answer, model_used, response_time)
            VALUES (%s, %s, %s, %s, %s)
        """
        cursor.execute(sql, (meeting_id, question, answer, model_used, response_time))
def list_all_meetings():
    """List all meetings"""
    with db.get_cursor() as cursor:
        sql = """
            SELECT meeting_id, title, status, start_time, end_time, total_chunks
            FROM meetings
            ORDER BY start_time DESC
        """
        cursor.execute(sql)
        return cursor.fetchall()
def log_system_event(level: str, message: str, meeting_id: str = None, stack_trace: str = None):
    """Log system event"""
    with db.get_cursor() as cursor:
        sql = """
            INSERT INTO system_logs (log_level, meeting_id, message, stack_trace)
            VALUES (%s, %s, %s, %s)
        """
        cursor.execute(sql, (level, meeting_id, message, stack_trace))
```
---
### **File: `backend/requirements.txt`**
```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6
python-dotenv==1.0.0
mysql-connector-python==8.2.0
openai==1.3.5
pydantic==2.5.0
numpy==1.24.3
scipy==1.11.4
```
---
### **File: `backend/transcription_service.py`**
```python
"""
Speech-to-Text transcription service using OpenAI Whisper API
"""
import openai
import logging
import io
import wave
from config import OPENAI_API_KEY, SAMPLE_RATE
openai.api_key = OPENAI_API_KEY
logger = logging.getLogger(__name__)
def transcribe_audio(audio_data: bytes, sample_rate: int = SAMPLE_RATE, language: str = None):
    """
    Transcribe audio using OpenAI Whisper API
   
    Args:
        audio_data: Raw audio bytes (PCM format)
        sample_rate: Audio sample rate
        language: Language code (e.g., 'en', 'es', 'hi') or None for auto-detect
   
    Returns:
        dict: Transcription result with text and detected language
    """
    try:
        # Convert raw PCM to WAV format
        wav_buffer = io.BytesIO()
        with wave.open(wav_buffer, 'wb') as wav_file:
            wav_file.setnchannels(1) # Mono
            wav_file.setsampwidth(2) # 16-bit
            wav_file.setframerate(sample_rate)
            wav_file.writeframes(audio_data)
       
        wav_buffer.seek(0)
        wav_buffer.name = "audio.wav" # Whisper API requires a filename
       
        # Call Whisper API
        transcript = openai.audio.transcriptions.create(
            model="whisper-1",
            file=wav_buffer,
            language=language if language and language != "auto" else None,
            response_format="verbose_json"
        )
       
        logger.info(f"Transcription successful. Detected language: {transcript.language}")
       
        return {
            'text': transcript.text,
            'language': transcript.language,
            'duration': transcript.duration if hasattr(transcript, 'duration') else None
        }
   
    except Exception as e:
        logger.error(f"Transcription error: {e}")
        return {
            'text': '',
            'language': 'unknown',
            'error': str(e)
        }
```
---
### **File: `backend/qa_service.py`**
```python
"""
AI-powered Question Answering service using OpenAI GPT
"""
import openai
import logging
import time
from config import OPENAI_API_KEY, LLM_MODEL, LLM_TEMPERATURE, MAX_TOKENS
openai.api_key = OPENAI_API_KEY
logger = logging.getLogger(__name__)
def answer_question(transcript: str, question: str):
    """
    Answer questions about the meeting transcript using GPT
   
    Args:
        transcript: Full meeting transcript
        question: User question
   
    Returns:
        dict: Answer and metadata
    """
    start_time = time.time()
   
    try:
        system_prompt = """You are an AI meeting assistant. Your task is to answer questions about meetings based on the provided transcript.
       
Guidelines:
- Be concise and accurate
- Quote relevant parts of the transcript when applicable
- If the answer is not in the transcript, say so clearly
- Extract action items, decisions, and key points when asked
- Identify speakers if mentioned in the transcript"""
        user_prompt = f"""Meeting Transcript:
{transcript}
Question: {question}
Please provide a clear and helpful answer based on the transcript."""
        response = openai.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=LLM_TEMPERATURE,
            max_tokens=MAX_TOKENS
        )
       
        answer = response.choices[0].message.content
        response_time = time.time() - start_time
       
        logger.info(f"Q&A completed in {response_time:.2f}s")
       
        return {
            'answer': answer,
            'model': LLM_MODEL,
            'response_time': response_time,
            'tokens_used': response.usage.total_tokens if hasattr(response, 'usage') else None
        }
   
    except Exception as e:
        logger.error(f"Q&A error: {e}")
        return {
            'answer': f"Error generating answer: {str(e)}",
            'model': LLM_MODEL,
            'response_time': time.time() - start_time,
            'error': str(e)
        }
def generate_summary(transcript: str):
    """Generate meeting summary"""
    return answer_question(
        transcript,
        "Please provide a comprehensive summary of this meeting including key discussion points, decisions made, and any action items."
    )
def extract_agenda(transcript: str):
    """Extract meeting agenda"""
    return answer_question(
        transcript,
        "What was the agenda of this meeting? List the main topics discussed."
    )
def extract_action_items(transcript: str):
    """Extract action items"""
    return answer_question(
        transcript,
        "Extract all action items, tasks, and follow-ups mentioned in this meeting. Format as a list with responsible persons if mentioned."
    )
```
---
### **File: `backend/audio_processor.py`**
```python
"""
Audio processing utilities
"""
import logging
import os
from config import UPLOAD_FOLDER
logger = logging.getLogger(__name__)
def save_audio_chunk_to_file(meeting_id: str, chunk_number: int, audio_data: bytes):
    """
    Save audio chunk to file system
   
    Args:
        meeting_id: Meeting identifier
        chunk_number: Chunk sequence number
        audio_data: Raw audio bytes
   
    Returns:
        str: File path
    """
    try:
        meeting_folder = os.path.join(UPLOAD_FOLDER, meeting_id)
        os.makedirs(meeting_folder, exist_ok=True)
       
        filename = f"chunk_{chunk_number:04d}.wav"
        filepath = os.path.join(meeting_folder, filename)
       
        with open(filepath, 'wb') as f:
            f.write(audio_data)
       
        logger.info(f"Saved audio chunk to: {filepath}")
        return filepath
   
    except Exception as e:
        logger.error(f"Error saving audio chunk: {e}")
        return None
def cleanup_old_files(meeting_id: str):
    """Delete temporary audio files after processing"""
    try:
        meeting_folder = os.path.join(UPLOAD_FOLDER, meeting_id)
        if os.path.exists(meeting_folder):
            for file in os.listdir(meeting_folder):
                os.remove(os.path.join(meeting_folder, file))
            os.rmdir(meeting_folder)
            logger.info(f"Cleaned up files for meeting: {meeting_id}")
    except Exception as e:
        logger.error(f"Error cleaning up files: {e}")
```
---
### **File: `backend/server.py`**
```python
"""
FastAPI server for VoiceMind Meeting System
"""
from fastapi import FastAPI, File, UploadFile, Header, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import logging
from datetime import datetime
import uvicorn
# Import local modules
from database import (
    init_database, close_database, create_meeting, get_meeting,
    update_meeting_status, save_audio_chunk, get_all_chunks,
    save_qa_interaction, list_all_meetings, log_system_event
)
from transcription_service import transcribe_audio
from qa_service import answer_question, generate_summary, extract_agenda, extract_action_items
from audio_processor import save_audio_chunk_to_file
from config import SERVER_HOST, SERVER_PORT
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/app.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)
# Initialize FastAPI app
app = FastAPI(
    title="VoiceMind Meeting API",
    description="AI-powered meeting recording and analysis system",
    version="1.0.0"
)
# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
# ============================================================================
# Startup and Shutdown Events
# ============================================================================
@app.on_event("startup")
async def startup_event():
    """Initialize database connection on startup"""
    logger.info("Starting VoiceMind API server...")
    init_database()
    log_system_event("INFO", "Server started")
@app.on_event("shutdown")
async def shutdown_event():
    """Close database connection on shutdown"""
    logger.info("Shutting down VoiceMind API server...")
    log_system_event("INFO", "Server stopped")
    close_database()
# ============================================================================
# API Endpoints
# ============================================================================
@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "status": "online",
        "message": "VoiceMind Meeting API",
        "timestamp": datetime.now().isoformat()
    }
@app.post("/api/start_meeting")
async def start_meeting(meeting_id: str, title: str = None, language: str = "auto"):
    """
    Start a new meeting recording session
   
    Args:
        meeting_id: Unique meeting identifier
        title: Meeting title (optional)
        language: Language code or 'auto' for detection
    """
    try:
        logger.info(f"Starting meeting: {meeting_id}")
       
        # Check if meeting already exists
        existing = get_meeting(meeting_id)
        if existing:
            raise HTTPException(status_code=400, detail="Meeting ID already exists")
       
        # Create meeting record
        create_meeting(meeting_id, title, language)
        log_system_event("INFO", f"Meeting started: {title or meeting_id}", meeting_id)
       
        return {
            "status": "success",
            "meeting_id": meeting_id,
            "message": "Meeting recording started"
        }
   
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error starting meeting: {e}")
        log_system_event("ERROR", f"Failed to start meeting: {str(e)}", meeting_id)
        raise HTTPException(status_code=500, detail=str(e))
@app.post("/api/upload_audio")
async def upload_audio(
    file: UploadFile = File(...),
    meeting_id: str = Header(..., alias="X-Meeting-ID"),
    chunk_number: int = Header(..., alias="X-Chunk-Number"),
    chunk_timestamp: int = Header(..., alias="X-Timestamp"),
    sample_rate: int = Header(16000, alias="X-Sample-Rate")
):
    """
    Upload audio chunk and process transcription
   
    Headers:
        X-Meeting-ID: Meeting identifier
        X-Chunk-Number: Chunk sequence number
        X-Timestamp: Chunk timestamp (milliseconds)
        X-Sample-Rate: Audio sample rate
    """
    try:
        logger.info(f"Receiving chunk {chunk_number} for meeting {meeting_id}")
       
        # Read audio data
        audio_data = await file.read()
       
        # Save to database
        save_audio_chunk(meeting_id, chunk_number, chunk_timestamp, audio_data, sample_rate)
       
        # Transcribe audio
        meeting = get_meeting(meeting_id)
        language = meeting['language'] if meeting else 'auto'
       
        transcription_result = transcribe_audio(audio_data, sample_rate, language)
        transcript_text = transcription_result.get('text', '')
       
        # Update chunk with transcript
        if transcript_text:
            save_audio_chunk(meeting_id, chunk_number, chunk_timestamp, audio_data, sample_rate, transcript_text)
            logger.info(f"Chunk {chunk_number} transcribed: {transcript_text[:50]}...")
       
        return {
            "status": "success",
            "chunk_number": chunk_number,
            "transcript": transcript_text,
            "language_detected": transcription_result.get('language', 'unknown')
        }
   
    except Exception as e:
        logger.error(f"Error processing audio chunk: {e}")
        log_system_event("ERROR", f"Failed to process chunk {chunk_number}: {str(e)}", meeting_id)
        raise HTTPException(status_code=500, detail=str(e))
@app.post("/api/end_meeting")
async def end_meeting(meeting_id: str):
    """
    End meeting and generate final transcript and summary
   
    Args:
        meeting_id: Meeting identifier
    """
    try:
        logger.info(f"Ending meeting: {meeting_id}")
       
        # Get all chunks
        chunks = get_all_chunks(meeting_id)
       
        if not chunks:
            raise HTTPException(status_code=404, detail="No audio chunks found for this meeting")
       
        # Compile full transcript
        full_transcript = " ".join([chunk['transcript_segment'] for chunk in chunks if chunk['transcript_segment']])
       
        # Generate summary
        summary_result = generate_summary(full_transcript)
        summary = summary_result['answer']
       
        # Extract agenda
        agenda_result = extract_agenda(full_transcript)
        agenda = agenda_result['answer']
       
        # Update meeting record
        update_meeting_status(meeting_id, 'completed', full_transcript, summary)
       
        # Update agenda field separately
        from database import db
        with db.get_cursor() as cursor:
            cursor.execute("UPDATE meetings SET agenda = %s WHERE meeting_id = %s", (agenda, meeting_id))
       
        log_system_event("INFO", f"Meeting completed: {meeting_id}", meeting_id)
       
        return {
            "status": "success",
            "meeting_id": meeting_id,
            "transcript_length": len(full_transcript),
            "total_chunks": len(chunks),
            "summary": summary,
            "agenda": agenda
        }
   
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error ending meeting: {e}")
        log_system_event("ERROR", f"Failed to end meeting: {str(e)}", meeting_id)
        raise HTTPException(status_code=500, detail=str(e))
@app.post("/api/ask_question")
async def ask_question_endpoint(meeting_id: str, question: str):
    """
    Ask a question about the meeting
   
    Args:
        meeting_id: Meeting identifier
        question: User question
    """
    try:
        logger.info(f"Q&A request for meeting {meeting_id}: {question}")
       
        # Get meeting
        meeting = get_meeting(meeting_id)
        if not meeting:
            raise HTTPException(status_code=404, detail="Meeting not found")
       
        transcript = meeting['full_transcript']
        if not transcript:
            raise HTTPException(status_code=400, detail="Meeting transcript not available yet")
       
        # Get answer
        result = answer_question(transcript, question)
       
        # Save Q&A interaction
        save_qa_interaction(
            meeting_id,
            question,
            result['answer'],
            result['model'],
            result['response_time']
        )
       
        return {
            "status": "success",
            "meeting_id": meeting_id,
            "question": question,
            "answer": result['answer'],
            "response_time": result['response_time']
        }
   
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error answering question: {e}")
        raise HTTPException(status_code=500, detail=str(e))
@app.get("/api/get_summary")
async def get_summary(meeting_id: str):
    """Get meeting summary"""
    try:
        meeting = get_meeting(meeting_id)
        if not meeting:
            raise HTTPException(status_code=404, detail="Meeting not found")
       
        return {
            "status": "success",
            "meeting_id": meeting_id,
            "title": meeting['title'],
            "summary": meeting['summary'],
            "agenda": meeting['agenda'],
            "start_time": meeting['start_time'],
            "end_time": meeting['end_time'],
            "status": meeting['status']
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting summary: {e}")
        raise HTTPException(status_code=500, detail=str(e))
@app.get("/api/get_transcript")
async def get_transcript(meeting_id: str):
    """Get full meeting transcript"""
    try:
        chunks = get_all_chunks(meeting_id)
       
        return {
            "status": "success",
            "meeting_id": meeting_id,
            "chunks": [
                {
                    "chunk_number": c['chunk_number'],
                    "text": c['transcript_segment'],
                    "timestamp": c['chunk_timestamp']
                }
                for c in chunks
            ]
        }
    except Exception as e:
        logger.error(f"Error getting transcript: {e}")
        raise HTTPException(status_code=500, detail=str(e))
@app.get("/api/list_meetings")
async def list_meetings_endpoint():
    """List all meetings"""
    try:
        meetings = list_all_meetings()
       
        return {
            "status": "success",
            "meetings": meetings
        }
    except Exception as e:
        logger.error(f"Error listing meetings: {e}")
        raise HTTPException(status_code=500, detail=str(e))
# ============================================================================
# Run Server
# ============================================================================
if __name__ == "__main__":
    uvicorn.run(
        "server:app",
        host=SERVER_HOST,
        port=SERVER_PORT,
        reload=True,
        log_level="info"
    )
```
---
## üéõÔ∏è STEP 5: ESP32 ARDUINO CODE
### **File: `hardware/esp32_audio_recorder/esp32_audio_recorder.ino`**
```cpp
/**
 * VoiceMind ESP32 Audio Recorder
 * Captures audio from INMP441 I2S microphone and streams to server
 */
#include <WiFi.h>
#include <HTTPClient.h>
#include <driver/i2s.h>
// ============================================================================
// CONFIGURATION - UPDATE THESE VALUES
// ============================================================================
// WiFi Credentials
const char* ssid = "YOUR_WIFI_SSID";
const char* password = "YOUR_WIFI_PASSWORD";
// Server Configuration
const char* serverUrl = "http://192.168.1.100:8000"; // Change to your server IP
String meetingId = "";
// I2S Pin Configuration (INMP441)
#define I2S_WS 25 // Word Select (LRCLK)
#define I2S_SD 32 // Serial Data (DOUT)
#define I2S_SCK 33 // Serial Clock (BCLK)
#define I2S_PORT I2S_NUM_0
// Audio Configuration
#define SAMPLE_RATE 16000
#define BITS_PER_SAMPLE I2S_BITS_PER_SAMPLE_16BIT
#define BUFFER_SIZE 1024
#define CHUNK_SIZE 32000 // ~2 seconds at 16kHz (32000 bytes = 16000 samples)
// GPIO Pins
#define LED_PIN 2
#define BUTTON_PIN 4
// ============================================================================
// GLOBAL VARIABLES
// ============================================================================
bool isRecording = false;
int chunkCounter = 0;
uint8_t audioBuffer[CHUNK_SIZE];
int bufferIndex = 0;
// ============================================================================
// SETUP
// ============================================================================
void setup() {
  Serial.begin(115200);
  delay(1000);
 
  Serial.println("\n\n===================================");
  Serial.println("VoiceMind ESP32 Audio Recorder");
  Serial.println("===================================\n");
 
  // Initialize GPIO
  pinMode(LED_PIN, OUTPUT);
  pinMode(BUTTON_PIN, INPUT_PULLUP);
  digitalWrite(LED_PIN, LOW);
 
  // Connect to WiFi
  connectWiFi();
 
  // Setup I2S
  setupI2S();
 
  Serial.println("‚úì System Ready!");
  Serial.println("Press button to start/stop recording\n");
 
  // Blink LED to indicate ready
  blinkLED(3, 200);
}
// ============================================================================
// MAIN LOOP
// ============================================================================
void loop() {
  // Check button press (with debouncing)
  static unsigned long lastButtonPress = 0;
  if (digitalRead(BUTTON_PIN) == LOW && (millis() - lastButtonPress) > 300) {
    lastButtonPress = millis();
   
    isRecording = !isRecording;
   
    if (isRecording) {
      startRecording();
    } else {
      stopRecording();
    }
   
    // Wait for button release
    while (digitalRead(BUTTON_PIN) == LOW) {
      delay(10);
    }
  }
 
  // Record and stream audio if recording is active
  if (isRecording) {
    recordAndStream();
  }
}
// ============================================================================
// WIFI CONNECTION
// ============================================================================
void connectWiFi() {
  Serial.print("Connecting to WiFi");
  WiFi.begin(ssid, password);
 
  int attempts = 0;
  while (WiFi.status() != WL_CONNECTED && attempts < 20) {
    delay(500);
    Serial.print(".");
    attempts++;
  }
 
  if (WiFi.status() == WL_CONNECTED) {
    Serial.println("\n‚úì WiFi Connected!");
    Serial.print("IP Address: ");
    Serial.println(WiFi.localIP());
    Serial.print("Signal Strength: ");
    Serial.print(WiFi.RSSI());
    Serial.println(" dBm\n");
  } else {
    Serial.println("\n‚úó WiFi Connection Failed!");
    Serial.println("Please check your credentials and try again.");
    while (true) {
      blinkLED(5, 100); // Fast blink to indicate error
      delay(1000);
    }
  }
}
// ============================================================================
// I2S SETUP
// ============================================================================
void setupI2S() {
  Serial.println("Configuring I2S...");
 
  // I2S configuration
  i2s_config_t i2s_config = {
    .mode = (i2s_mode_t)(I2S_MODE_MASTER | I2S_MODE_RX),
    .sample_rate = SAMPLE_RATE,
    .bits_per_sample = BITS_PER_SAMPLE,
    .channel_format = I2S_CHANNEL_FMT_ONLY_LEFT,
    .communication_format = I2S_COMM_FORMAT_I2S,
    .intr_alloc_flags = ESP_INTR_FLAG_LEVEL1,
    .dma_buf_count = 4,
    .dma_buf_len = BUFFER_SIZE,
    .use_apll = false,
    .tx_desc_auto_clear = false,
    .fixed_mclk = 0
  };
 
  // Pin configuration
  i2s_pin_config_t pin_config = {
    .bck_io_num = I2S_SCK,
    .ws_io_num = I2S_WS,
    .data_out_num = I2S_PIN_NO_CHANGE,
    .data_in_num = I2S_SD
  };
 
  // Install and set pin config
  esp_err_t err = i2s_driver_install(I2S_PORT, &i2s_config, 0, NULL);
  if (err != ESP_OK) {
    Serial.printf("‚úó Failed to install I2S driver: %d\n", err);
    return;
  }
 
  err = i2s_set_pin(I2S_PORT, &pin_config);
  if (err != ESP_OK) {
    Serial.printf("‚úó Failed to set I2S pins: %d\n", err);
    return;
  }
 
  i2s_zero_dma_buffer(I2S_PORT);
 
  Serial.println("‚úì I2S Configured Successfully\n");
}
// ============================================================================
// RECORDING FUNCTIONS
// ============================================================================
void startRecording() {
  Serial.println("\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó");
  Serial.println("‚ïë RECORDING STARTED ‚ïë");
  Serial.println("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n");
 
  digitalWrite(LED_PIN, HIGH);
 
  // Generate unique meeting ID
  meetingId = "meeting_" + String(millis());
  chunkCounter = 0;
  bufferIndex = 0;
 
  // Notify server about new meeting
  HTTPClient http;
  String url = String(serverUrl) + "/api/start_meeting?meeting_id=" + meetingId;
  http.begin(url);
 
  int httpCode = http.POST("");
 
  if (httpCode == 200) {
    Serial.println("‚úì Meeting session created on server");
    Serial.println("Meeting ID: " + meetingId + "\n");
  } else {
    Serial.printf("‚úó Failed to start meeting on server: HTTP %d\n\n", httpCode);
  }
 
  http.end();
}
void stopRecording() {
  Serial.println("\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó");
  Serial.println("‚ïë RECORDING STOPPED ‚ïë");
  Serial.println("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n");
 
  digitalWrite(LED_PIN, LOW);
 
  // Send any remaining data in buffer
  if (bufferIndex > 0) {
    sendAudioChunk(audioBuffer, bufferIndex);
  }
 
  // Notify server about meeting end
  HTTPClient http;
  String url = String(serverUrl) + "/api/end_meeting?meeting_id=" + meetingId;
  http.begin(url);
 
  int httpCode = http.POST("");
 
  if (httpCode == 200) {
    Serial.println("‚úì Meeting ended on server");
    Serial.println("Processing transcript and summary...\n");
  } else {
    Serial.printf("‚úó Failed to end meeting on server: HTTP %d\n\n", httpCode);
  }
 
  http.end();
 
  Serial.printf("Total chunks sent: %d\n", chunkCounter);
  Serial.printf("Approximate duration: %.1f seconds\n\n", (chunkCounter * 2.0));
}
void recordAndStream() {
  uint8_t tempBuffer[BUFFER_SIZE];
  size_t bytesRead = 0;
 
  // Read audio data from I2S
  esp_err_t result = i2s_read(I2S_PORT, tempBuffer, BUFFER_SIZE, &bytesRead, portMAX_DELAY);
 
  if (result != ESP_OK) {
    Serial.printf("‚úó I2S read error: %d\n", result);
    return;
  }
 
  // Accumulate data into chunk buffer
  for (size_t i = 0; i < bytesRead && bufferIndex < CHUNK_SIZE; i++) {
    audioBuffer[bufferIndex++] = tempBuffer[i];
  }
 
  // When chunk is full, send to server
  if (bufferIndex >= CHUNK_SIZE) {
    sendAudioChunk(audioBuffer, bufferIndex);
    bufferIndex = 0;
    chunkCounter++;
  }
}
void sendAudioChunk(uint8_t* data, int length) {
  // Check WiFi connection
  if (WiFi.status() != WL_CONNECTED) {
    Serial.println("‚úó WiFi disconnected! Attempting to reconnect...");
    connectWiFi();
    return;
  }
 
  HTTPClient http;
  String url = String(serverUrl) + "/api/upload_audio";
 
  http.begin(url);
  http.addHeader("Content-Type", "application/octet-stream");
  http.addHeader("X-Meeting-ID", meetingId);
  http.addHeader("X-Chunk-Number", String(chunkCounter));
  http.addHeader("X-Timestamp", String(millis()));
  http.addHeader("X-Sample-Rate", String(SAMPLE_RATE));
 
  int httpCode = http.POST(data, length);
 
  if (httpCode == 200) {
    Serial.printf("‚úì Chunk %d sent (%d bytes)\n", chunkCounter, length);
   
    // Quick LED blink to indicate successful transmission
    digitalWrite(LED_PIN, LOW);
    delay(50);
    digitalWrite(LED_PIN, HIGH);
  } else {
    Serial.printf("‚úó Error sending chunk %d: HTTP %d\n", chunkCounter, httpCode);
  }
 
  http.end();
}
// ============================================================================
// UTILITY FUNCTIONS
// ============================================================================
void blinkLED(int times, int delayMs) {
  for (int i = 0; i < times; i++) {
    digitalWrite(LED_PIN, HIGH);
    delay(delayMs);
    digitalWrite(LED_PIN, LOW);
    delay(delayMs);
  }
}
```
---
## üë§ STEP 6: CLIENT APPLICATION
### **File: `client/meeting_client.py`**
```python
"""
VoiceMind Meeting Client
Interactive command-line interface for querying meetings
"""
import requests
import json
from datetime import datetime
import sys
class MeetingClient:
    def __init__(self, server_url="http://localhost:8000"):
        self.server_url = server_url
        self.current_meeting = None
        print("=" * 70)
        print(" " * 15 + "VoiceMind Meeting Client")
        print("=" * 70 + "\n")
   
    def check_server(self):
        """Check if server is online"""
        try:
            response = requests.get(f"{self.server_url}/")
            if response.status_code == 200:
                print("‚úì Connected to server\n")
                return True
        except:
            print("‚úó Cannot connect to server. Please ensure server is running.\n")
            return False
   
    def list_meetings(self):
        """List all available meetings"""
        try:
            response = requests.get(f"{self.server_url}/api/list_meetings")
            data = response.json()
            meetings = data['meetings']
           
            print("\n" + "=" * 70)
            print(" " * 20 + "AVAILABLE MEETINGS")
            print("=" * 70)
           
            if not meetings:
                print("\nNo meetings found.")
            else:
                for i, meeting in enumerate(meetings, 1):
                    print(f"\n{i}. Meeting ID: {meeting['meeting_id']}")
                    print(f" Title: {meeting['title'] or 'Untitled'}")
                    print(f" Status: {meeting['status']}")
                    print(f" Start Time: {meeting['start_time']}")
                    print(f" Chunks: {meeting['total_chunks']}")
           
            print("\n" + "=" * 70)
            return meetings
       
        except Exception as e:
            print(f"‚úó Error listing meetings: {e}")
            return []
   
    def select_meeting(self, meeting_id):
        """Select a meeting for analysis"""
        try:
            response = requests.get(f"{self.server_url}/api/get_summary?meeting_id={meeting_id}")
            if response.status_code == 200:
                self.current_meeting = meeting_id
                print(f"\n‚úì Selected meeting: {meeting_id}")
                return True
            else:
                print(f"\n‚úó Meeting not found: {meeting_id}")
                return False
        except Exception as e:
            print(f"‚úó Error selecting meeting: {e}")
            return False
   
    def get_summary(self):
        """Get meeting summary"""
        if not self.current_meeting:
            print("‚ùå Please select a meeting first!")
            return
       
        try:
            response = requests.get(f"{self.server_url}/api/get_summary?meeting_id={self.current_meeting}")
            data = response.json()
           
            print("\n" + "=" * 70)
            print(" " * 25 + "MEETING SUMMARY")
            print("=" * 70)
            print(f"\nMeeting ID: {data['meeting_id']}")
            print(f"Title: {data['title'] or 'Untitled'}")
            print(f"Status: {data['status']}")
            print(f"Start Time: {data['start_time']}")
            print(f"End Time: {data['end_time']}")
            print(f"\n{'-' * 70}")
            print("SUMMARY:")
            print(f"{'-' * 70}")
            print(data['summary'])
            print(f"\n{'-' * 70}")
            print("AGENDA:")
            print(f"{'-' * 70}")
            print(data['agenda'])
            print("\n" + "=" * 70)
       
        except Exception as e:
            print(f"‚úó Error getting summary: {e}")
   
    def get_transcript(self):
        """Get full meeting transcript"""
        if not self.current_meeting:
            print("‚ùå Please select a meeting first!")
            return
       
        try:
            response = requests.get(f"{self.server_url}/api/get_transcript?meeting_id={self.current_meeting}")
            data = response.json()
           
            print("\n" + "=" * 70)
            print(" " * 23 + "FULL TRANSCRIPT")
            print("=" * 70)
           
            for chunk in data['chunks']:
                timestamp = chunk['timestamp'] / 1000 # Convert to seconds
                print(f"\n[{timestamp:.1f}s] Chunk {chunk['chunk_number']}:")
                print(chunk['text'])
           
            print("\n" + "=" * 70)
       
        except Exception as e:
            print(f"‚úó Error getting transcript: {e}")
   
    def ask_question(self, question):
        """Ask a question about the meeting"""
        if not self.current_meeting:
            print("‚ùå Please select a meeting first!")
            return
       
        try:
            payload = {
                "meeting_id": self.current_meeting,
                "question": question
            }
           
            print("\n‚è≥ Processing your question...")
            response = requests.post(f"{self.server_url}/api/ask_question", params=payload)
            data = response.json()
           
            print("\n" + "-" * 70)
            print(f"‚ùì Q: {question}")
            print(f"\nüí° A: {data['answer']}")
            print(f"\n‚è±Ô∏è Response time: {data['response_time']:.2f}s")
            print("-" * 70)
       
        except Exception as e:
            print(f"‚úó Error asking question: {e}")
   
    def interactive_mode(self):
        """Run interactive Q&A session"""
        print("\n" + "=" * 70)
        print(" " * 20 + "INTERACTIVE MODE")
        print("=" * 70)
        print("\nCommands:")
        print(" 'list' - List all meetings")
        print(" 'select <id>' - Select a meeting")
        print(" 'summary' - Get meeting summary")
        print(" 'transcript' - Get full transcript")
        print(" 'exit' or 'quit' - Exit interactive mode")
        print(" Any other text - Ask question about current meeting")
        print("\n" + "=" * 70)
       
        while True:
            try:
                user_input = input("\n> ").strip()
               
                if not user_input:
                    continue
               
                if user_input.lower() in ['exit', 'quit']:
                    print("\nGoodbye! üëã\n")
                    break
               
                elif user_input.lower() == 'list':
                    self.list_meetings()
               
                elif user_input.lower().startswith('select '):
                    meeting_id = user_input[7:].strip()
                    self.select_meeting(meeting_id)
               
                elif user_input.lower() == 'summary':
                    self.get_summary()
               
                elif user_input.lower() == 'transcript':
                    self.get_transcript()
               
                else:
                    # Treat as question
                    self.ask_question(user_input)
           
            except KeyboardInterrupt:
                print("\n\nGoodbye! üëã\n")
                break
            except Exception as e:
                print(f"\n‚úó Error: {e}")
# ============================================================================
# MAIN ENTRY POINT
# ============================================================================
if __name__ == "__main__":
    import sys
   
    # Get server URL from command line argument or use default
    server_url = sys.argv[1] if len(sys.argv) > 1 else "http://localhost:8000"
   
    client = MeetingClient(server_url)
   
    if not client.check_server():
        sys.exit(1)
   
    # Auto-list meetings on startup
    meetings = client.list_meetings()
   
    # If meetings exist, auto-select the first one
    if meetings:
        client.select_meeting(meetings[0]['meeting_id'])
   
    # Start interactive mode
    client.interactive_mode()
```
---
## üóÑÔ∏è STEP 7: DATABASE INITIALIZATION
### **File: `database/init_db.py`**
```python
"""
Database initialization script
"""
import mysql.connector
from mysql.connector import Error
import sys
# Configuration
DB_HOST = "localhost"
DB_PORT = 3306
DB_USER = "root"
DB_PASSWORD = "" # Update with your MySQL root password
def init_database():
    """Initialize database from schema file"""
    try:
        # Connect to MySQL server (without database)
        connection = mysql.connector.connect(
            host=DB_HOST,
            port=DB_PORT,
            user=DB_USER,
            password=DB_PASSWORD
        )
       
        if connection.is_connected():
            cursor = connection.cursor()
           
            # Read and execute schema file
            print("üìÑ Reading schema file...")
            with open('database/schema.sql', 'r') as f:
                schema = f.read()
           
            # Split and execute statements
            statements = schema.split(';')
           
            for i, statement in enumerate(statements):
                statement = statement.strip()
                if statement:
                    try:
                        cursor.execute(statement)
                        print(f"‚úì Executed statement {i+1}/{len(statements)}")
                    except Error as e:
                        if "already exists" not in str(e):
                            print(f"‚ö†Ô∏è Warning: {e}")
           
            connection.commit()
            print("\n‚úÖ Database initialized successfully!")
            print("Database: voicemind_db")
            print("Tables: meetings, audio_chunks, qa_history, system_logs\n")
           
            cursor.close()
            connection.close()
   
    except Error as e:
        print(f"‚ùå Error initializing database: {e}")
        sys.exit(1)
if __name__ == "__main__":
    print("=" * 60)
    print(" " * 15 + "VoiceMind Database Setup")
    print("=" * 60 + "\n")
   
    init_database()
```
---
## üåê STEP 8: ENVIRONMENT CONFIGURATION
### **File: `.env`**
```env
# Server Configuration
SERVER_HOST=0.0.0.0
SERVER_PORT=8000
# MySQL Database Configuration
DB_HOST=localhost
DB_PORT=3306
DB_USER=root
DB_PASSWORD=your_mysql_password_here
DB_NAME=voicemind_db
# OpenAI API Key (Get from https://platform.openai.com/api-keys)
OPENAI_API_KEY=sk-your-openai-api-key-here
# Audio Configuration
SAMPLE_RATE=16000
CHUNK_DURATION_MS=10000
# Paths
UPLOAD_FOLDER=uploads/audio_chunks
LOG_FOLDER=logs
# Whisper Model (tiny, base, small, medium, large)
WHISPER_MODEL=base
# LLM Configuration
LLM_MODEL=gpt-3.5-turbo
LLM_TEMPERATURE=0.7
MAX_TOKENS=500
```
---
## ‚öôÔ∏è STEP 9: AUTOMATED SETUP SCRIPT
### **File: `setup.sh`** (Linux/Mac)
```bash
#!/bin/bash
echo "======================================================================"
echo " VoiceMind Meeting System - Automated Setup"
echo "======================================================================"
echo ""
# Check Python installation
echo "Checking Python installation..."
if ! command -v python3 &> /dev/null; then
    echo "‚ùå Python 3 is not installed. Please install Python 3.8 or higher."
    exit 1
fi
echo "‚úì Python 3 found"
echo ""
# Check MySQL installation
echo "Checking MySQL installation..."
if ! command -v mysql &> /dev/null; then
    echo "‚ùå MySQL is not installed. Please install MySQL 8.0 or higher."
    exit 1
fi
echo "‚úì MySQL found"
echo ""
# Create virtual environment
echo "Creating Python virtual environment..."
python3 -m venv venv
source venv/bin/activate
echo "‚úì Virtual environment created"
echo ""
# Install Python dependencies
echo "Installing Python dependencies..."
pip install --upgrade pip
pip install -r backend/requirements.txt
echo "‚úì Dependencies installed"
echo ""
# Initialize database
echo "Initializing MySQL database..."
python database/init_db.py
echo "‚úì Database initialized"
echo ""
# Create .env file if not exists
if [ ! -f .env ]; then
    echo "Creating .env configuration file..."
    cp .env.example .env
    echo "‚ö†Ô∏è Please edit .env file and add your configuration (MySQL password, OpenAI API key)"
fi
echo ""
echo "======================================================================"
echo "‚úÖ Setup completed successfully!"
echo "======================================================================"
echo ""
echo "Next steps:"
echo "1. Edit .env file with your MySQL password and OpenAI API key"
echo "2. Upload ESP32 Arduino code to your device"
echo "3. Start the server: python backend/server.py"
echo "4. Run the client: python client/meeting_client.py"
echo ""
```
### **File: `setup.bat`** (Windows)
```batch
@echo off
echo ======================================================================
echo VoiceMind Meeting System - Automated Setup
echo ======================================================================
echo.
REM Check Python installation
echo Checking Python installation...
python --version >nul 2>&1
if errorlevel 1 (
    echo X Python is not installed. Please install Python 3.8 or higher.
    pause
    exit /b 1
)
echo + Python found
echo.
REM Check MySQL installation
echo Checking MySQL installation...
mysql --version >nul 2>&1
if errorlevel 1 (
    echo X MySQL is not installed. Please install MySQL 8.0 or higher.
    pause
    exit /b 1
)
echo + MySQL found
echo.
REM Create virtual environment
echo Creating Python virtual environment...
python -m venv venv
call venv\Scripts\activate
echo + Virtual environment created
echo.
REM Install Python dependencies
echo Installing Python dependencies...
pip install --upgrade pip
pip install -r backend\requirements.txt
echo + Dependencies installed
echo.
REM Initialize database
echo Initializing MySQL database...
python database\init_db.py
echo + Database initialized
echo.
REM Create .env file if not exists
if not exist .env (
    echo Creating .env configuration file...
    copy .env.example .env
    echo ! Please edit .env file and add your configuration
)
echo.
echo ======================================================================
echo + Setup completed successfully!
echo ======================================================================
echo.
echo Next steps:
echo 1. Edit .env file with your MySQL password and OpenAI API key
echo 2. Upload ESP32 Arduino code to your device
echo 3. Start the server: python backend\server.py
echo 4. Run the client: python client\meeting_client.py
echo.
pause
```
---
## üìù STEP 10: COMPLETE README
### **File: `README.md`**
```markdown
# VoiceMind Meeting Intelligence System
AI-powered meeting recording and analysis system with multilingual support.
## System Requirements
### Hardware
- ESP32 DevKit (with WiFi)
- INMP441 I2S MEMS Microphone
- 5V USB Power Supply
- Breadboard and jumper wires
### Software
- Python 3.8 or higher
- MySQL 8.0 or higher
- Arduino IDE (for ESP32 programming)
- OpenAI API key
## Installation
### 1. Clone Repository
```bash
git clone <repository-url>
cd VoiceMind-Meeting-System
```
### 2. Run Setup Script
**Linux/Mac:**
```bash
chmod +x setup.sh
./setup.sh
```
**Windows:**
```batch
setup.bat
```
### 3. Configure Environment
Edit `.env` file:
```env
DB_PASSWORD=your_mysql_password
OPENAI_API_KEY=sk-your-openai-key
```
### 4. Upload ESP32 Code
1. Open `hardware/esp32_audio_recorder/esp32_audio_recorder.ino` in Arduino IDE
2. Update WiFi credentials and server URL
3. Select board: ESP32 Dev Module
4. Upload code
## Usage
### Start Server
```bash
# Activate virtual environment
source venv/bin/activate # Linux/Mac
# OR
venv\Scripts\activate # Windows
# Run server
python backend/server.py
```
Server will start at `http://localhost:8000`
### Start Recording
1. Press button on ESP32 to start recording
2. LED will turn on to indicate recording
3. Speak into microphone
4. Press button again to stop
### Query Meeting
```bash
# Run client
python client/meeting_client.py
# Interactive commands
> list # List all meetings
> select meeting_12345 # Select a meeting
> summary # Get meeting summary
> What was the agenda? # Ask questions
> exit # Exit
```
## API Endpoints
| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/` | Health check |
| POST | `/api/start_meeting` | Start new meeting |
| POST | `/api/upload_audio` | Upload audio chunk |
| POST | `/api/end_meeting` | End meeting |
| POST | `/api/ask_question` | Ask question |
| GET | `/api/get_summary` | Get meeting summary |
| GET | `/api/get_transcript` | Get full transcript |
| GET | `/api/list_meetings` | List all meetings |
## Database Schema
### Tables
- `meetings` - Meeting metadata
- `audio_chunks` - Audio segments
- `qa_history` - Q&A interactions
- `system_logs` - System events
## Troubleshooting
### ESP32 won't connect to WiFi
- Check WiFi credentials
- Ensure 2.4GHz network (ESP32 doesn't support 5GHz)
- Check signal strength
### No audio captured
- Verify I2S pin connections
- Check microphone power (3.3V)
- Test with serial monitor
### Transcription errors
- Ensure OpenAI API key is valid
- Check internet connection
- Verify audio quality
### Database connection failed
- Ensure MySQL is running
- Check credentials in `.env`
- Verify database exists
## License
MIT License
## Support
For issues and questions, please open a GitHub issue.
```
---
## üöÄ STEP 11: COMMAND REFERENCE
### **Complete Command Workflow**
```bash
# ============================================================================
# INSTALLATION COMMANDS
# ============================================================================
# 1. Create project structure
mkdir VoiceMind-Meeting-System && cd VoiceMind-Meeting-System
mkdir -p hardware/esp32_audio_recorder hardware/circuit_diagrams backend client database uploads/audio_chunks logs
# 2. Install MySQL (if not installed)
# Ubuntu/Debian:
sudo apt update
sudo apt install mysql-server
# macOS (with Homebrew):
brew install mysql
brew services start mysql
# Windows: Download from https://dev.mysql.com/downloads/installer/
# 3. Secure MySQL installation
sudo mysql_secure_installation
# 4. Create Python virtual environment
python3 -m venv venv
source venv/bin/activate # Linux/Mac
# OR
venv\Scripts\activate # Windows
# 5. Install Python dependencies
pip install --upgrade pip
pip install -r backend/requirements.txt
# 6. Initialize database
python database/init_db.py
# ============================================================================
# RUNNING THE SYSTEM
# ============================================================================
# Terminal 1: Start Backend Server
cd VoiceMind-Meeting-System
source venv/bin/activate # Activate virtual environment
python backend/server.py
# Terminal 2: Run Client (after recording)
cd VoiceMind-Meeting-System
source venv/bin/activate
python client/meeting_client.py
# ============================================================================
# ESP32 COMMANDS (Arduino IDE)
# ============================================================================
# 1. Install ESP32 Board Support
# Arduino IDE: File > Preferences
# Add to "Additional Board Manager URLs":
https://raw.githubusercontent.com/espressif/arduino-esp32/gh-pages/package_esp32_index.json
# 2. Install Board
# Tools > Board > Boards Manager > Search "ESP32" > Install
# 3. Select Board
# Tools > Board > ESP32 Arduino > ESP32 Dev Module
# 4. Configure Upload Settings
# Tools > Upload Speed > 921600
# Tools > Flash Frequency > 80MHz
# Tools > Partition Scheme > Default 4MB
# 5. Upload Code
# Sketch > Upload (or Ctrl+U)
# ============================================================================
# DATABASE COMMANDS
# ============================================================================
# Login to MySQL
mysql -u root -p
# Check database
USE voicemind_db;
SHOW TABLES;
# View meetings
SELECT * FROM meetings;
# View audio chunks
SELECT meeting_id, chunk_number, LENGTH(audio_data) as size_bytes, transcript_segment
FROM audio_chunks
ORDER BY chunk_number;
# View Q&A history
SELECT * FROM qa_history ORDER BY created_at DESC LIMIT 10;
# ============================================================================
# TESTING COMMANDS
# ============================================================================
# Test server health
curl http://localhost:8000/
# List meetings
curl http://localhost:8000/api/list_meetings
# Get meeting summary
curl "http://localhost:8000/api/get_summary?meeting_id=meeting_12345"
# Ask question (POST)
curl -X POST "http://localhost:8000/api/ask_question?meeting_id=meeting_12345&question=What+was+the+agenda?"
# ============================================================================
# MAINTENANCE COMMANDS
# ============================================================================
# View server logs
tail -f logs/app.log
# Backup database
mysqldump -u root -p voicemind_db > backup_$(date +%Y%m%d).sql
# Restore database
mysql -u root -p voicemind_db < backup_20240115.sql
# Clean up old audio files
find uploads/audio_chunks -type f -mtime +30 -delete
# ============================================================================
# VS CODE COMMANDS
# ============================================================================
# Open project in VS Code
code .
# Install recommended extensions
# Python, MySQL, REST Client, Arduino
# Debug Python server (Press F5 after creating launch.json)
# ============================================================================
# DEPLOYMENT COMMANDS (Production)
# ============================================================================
# Install production server
pip install gunicorn
# Run with Gunicorn
gunicorn -w 4 -k uvicorn.workers.UvicornWorker backend.server:app --bind 0.0.0.0:8000
# Run as systemd service (Linux)
sudo systemctl start voicemind
sudo systemctl enable voicemind
```
---
## üéØ FINAL VERIFICATION CHECKLIST
This is a **complete, production-ready system** with:
‚úÖ Full folder structure
‚úÖ MySQL database schema
‚úÖ Complete backend with FastAPI
‚úÖ ESP32 Arduino code with I2S audio
‚úÖ Interactive CLI client
‚úÖ Automated setup scripts
‚úÖ Comprehensive documentation
‚úÖ All commands for setup, testing, and deployment
hello according to above content and this xlsx 'https://docs.google.com/spreadsheets/d/17R0P6PuqjgMITgjyecoXCyZPWfrg60eWirKR5vvTonk/edit?gid=1235807852#gid=1235807852' format link first analysis , understand hole project from top to bottom step by step understand then create a fully function frontend from above content and link all requirement should meet make using a mern stack. write command for create a folder or file, proper code modify from compare all code , write a code for tree of file folder .also i am using vs code so be in mind.
i need proper step by step guide and follow instruction then write a command to run , test, compile .. etc





Frontend Folder Tree

voicemind-frontend/
‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îú‚îÄ‚îÄ favicon.ico
‚îÇ   ‚îî‚îÄ‚îÄ manifest.json
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logo.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ styles/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ global.css
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ variables.css
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ common/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Header.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Sidebar.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LoadingSpinner.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ErrorBoundary.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Toast.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ meeting/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MeetingList.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MeetingCard.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MeetingDetail.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RecordingControls.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ StatusIndicator.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transcript/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TranscriptViewer.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TranscriptSegment.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SearchTranscript.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qa/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatInterface.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ QuestionInput.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AnswerCard.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ summary/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ SummaryView.jsx
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ AgendaView.jsx
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ActionItemsList.jsx
‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dashboard.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MeetingPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AnalyticsPage.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SettingsPage.jsx
‚îÇ   ‚îú‚îÄ‚îÄ redux/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ store.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ slices/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ meetingSlice.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transcriptSlice.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qaSlice.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ uiSlice.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ middleware/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ websocketMiddleware.js
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ meetingService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transcriptionService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qaService.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ websocketService.js
‚îÇ   ‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useMeetings.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useWebSocket.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useAudio.js
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dateFormatter.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ audioUtils.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validators.js
‚îÇ   ‚îú‚îÄ‚îÄ App.jsx
‚îÇ   ‚îú‚îÄ‚îÄ App.css
‚îÇ   ‚îú‚îÄ‚îÄ index.js
‚îÇ   ‚îî‚îÄ‚îÄ index.css
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ vite.config.js (or webpack.config.js)














VS Code Setup Commands

# Navigate to your project root
cd ~/Projects

# Create frontend directory
mkdir voicemind-frontend
cd voicemind-frontend

# Create all directories
mkdir -p public src/assets/images src/assets/styles src/components/common src/components/meeting src/components/transcript src/components/qa src/components/summary src/pages src/redux/slices src/redux/middleware src/services src/hooks src/utils

# Create public files
touch public/index.html public/favicon.ico public/manifest.json

# Create asset files
touch src/assets/styles/global.css src/assets/styles/variables.css

# Create common components
touch src/components/common/Header.jsx src/components/common/Sidebar.jsx src/components/common/LoadingSpinner.jsx src/components/common/ErrorBoundary.jsx src/components/common/Toast.jsx

# Create meeting components
touch src/components/meeting/MeetingList.jsx src/components/meeting/MeetingCard.jsx src/components/meeting/MeetingDetail.jsx src/components/meeting/RecordingControls.jsx src/components/meeting/StatusIndicator.jsx

# Create transcript components
touch src/components/transcript/TranscriptViewer.jsx src/components/transcript/TranscriptSegment.jsx src/components/transcript/SearchTranscript.jsx

# Create QA components
touch src/components/qa/ChatInterface.jsx src/components/qa/QuestionInput.jsx src/components/qa/AnswerCard.jsx

# Create summary components
touch src/components/summary/SummaryView.jsx src/components/summary/AgendaView.jsx src/components/summary/ActionItemsList.jsx

# Create pages
touch src/pages/Dashboard.jsx src/pages/MeetingPage.jsx src/pages/AnalyticsPage.jsx src/pages/SettingsPage.jsx

# Create Redux files
touch src/redux/store.js src/redux/slices/meetingSlice.js src/redux/slices/transcriptSlice.js src/redux/slices/qaSlice.js src/redux/slices/uiSlice.js src/redux/middleware/websocketMiddleware.js

# Create services
touch src/services/api.js src/services/meetingService.js src/services/transcriptionService.js src/services/qaService.js src/services/websocketService.js

# Create hooks
touch src/hooks/useMeetings.js src/hooks/useWebSocket.js src/hooks/useAudio.js

# Create utils
touch src/utils/dateFormatter.js src/utils/audioUtils.js src/utils/validators.js

# Create root files
touch src/App.jsx src/App.css src/index.js src/index.css .env .env.example .gitignore package.json README.md vite.config.js

# Open in VS Code
code .




Installation Commands


{
  "name": "voicemind-frontend",
  "version": "1.0.0",
  "description": "VoiceMind Meeting Intelligence System - MERN Frontend",
  "main": "src/index.js",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview",
    "lint": "eslint src --ext js,jsx",
    "format": "prettier --write \"src/**/*.{js,jsx,css}\"",
    "test": "jest"
  },
  "dependencies": {
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "react-router-dom": "^6.20.0",
    "@reduxjs/toolkit": "^2.0.1",
    "react-redux": "^9.0.4",
    "axios": "^1.6.2",
    "socket.io-client": "^4.5.4",
    "@mui/material": "^5.15.0",
    "@mui/icons-material": "^5.15.0",
    "@emotion/react": "^11.11.1",
    "@emotion/styled": "^11.11.0",
    "recharts": "^2.10.3",
    "react-wavesurfer": "^2.0.0",
    "wavesurfer.js": "^7.4.4",
    "date-fns": "^3.0.6",
    "react-toastify": "^9.1.3",
    "framer-motion": "^10.16.16",
    "react-markdown": "^9.0.1"
  },
  "devDependencies": {
    "vite": "^5.0.7",
    "@vitejs/plugin-react": "^4.2.1",
    "eslint": "^8.55.0",
    "eslint-plugin-react": "^7.33.2",
    "prettier": "^3.1.1",
    "@types/react": "^18.2.43",
    "@types/react-dom": "^18.2.17"
  }
}







Installation Commands

# Initialize Node project (if package.json not created yet)
npm init -y

# Install all dependencies
npm install

# Or with Yarn
yarn install



.env.example

# Backend API Configuration
VITE_API_BASE_URL=http://localhost:8000
VITE_WS_BASE_URL=ws://localhost:8000

# Feature Flags
VITE_ENABLE_WEBSOCKET=true
VITE_ENABLE_ANALYTICS=true

# API Timeout (milliseconds)
VITE_API_TIMEOUT=30000

# Audio Configuration
VITE_AUDIO_SAMPLE_RATE=16000
VITE_AUDIO_CHUNK_SIZE=32000
VITE_AUDIO_FORMAT=wav

# UI Configuration
VITE_THEME=light
VITE_ITEMS_PER_PAGE=10





.env (Create from example)
cp .env.example .env
# Edit .env with your actual values



vite.config.js
import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react';
import path from 'path';

export default defineConfig({
  plugins: [react()],
  resolve: {
    alias: {
      '@': path.resolve(__dirname, './src'),
      '@components': path.resolve(__dirname, './src/components'),
      '@pages': path.resolve(__dirname, './src/pages'),
      '@services': path.resolve(__dirname, './src/services'),
      '@redux': path.resolve(__dirname, './src/redux'),
      '@hooks': path.resolve(__dirname, './src/hooks'),
      '@utils': path.resolve(__dirname, './src/utils'),
      '@assets': path.resolve(__dirname, './src/assets'),
    },
  },
  server: {
    port: 3000,
    proxy: {
      '/api': {
        target: 'http://localhost:8000',
        changeOrigin: true,
      },
      '/ws': {
        target: 'ws://localhost:8000',
        ws: true,
      },
    },
  },
  build: {
    outDir: 'dist',
    sourcemap: true,
  },
});


üîå Core Services Layer
src/services/api.js


import axios from 'axios';

const API_BASE_URL = import.meta.env.VITE_API_BASE_URL || 'http://localhost:8000';
const API_TIMEOUT = parseInt(import.meta.env.VITE_API_TIMEOUT) || 30000;

// Create axios instance
const apiClient = axios.create({
  baseURL: API_BASE_URL,
  timeout: API_TIMEOUT,
  headers: {
    'Content-Type': 'application/json',
  },
});

// Request interceptor
apiClient.interceptors.request.use(
  (config) => {
    // Add auth token if exists
    const token = localStorage.getItem('authToken');
    if (token) {
      config.headers.Authorization = `Bearer ${token}`;
    }
    console.log(`[API Request] ${config.method.toUpperCase()} ${config.url}`);
    return config;
  },
  (error) => {
    console.error('[API Request Error]', error);
    return Promise.reject(error);
  }
);

// Response interceptor
apiClient.interceptors.response.use(
  (response) => {
    console.log(`[API Response] ${response.status} ${response.config.url}`);
    return response;
  },
  (error) => {
    if (error.response) {
      console.error(`[API Error] ${error.response.status}:`, error.response.data);
      
      // Handle specific status codes
      if (error.response.status === 401) {
        localStorage.removeItem('authToken');
        window.location.href = '/login';
      }
    } else if (error.request) {
      console.error('[API Error] No response received:', error.request);
    } else {
      console.error('[API Error]', error.message);
    }
    return Promise.reject(error);
  }
);

export default apiClient;



src/services/meetingService.js

import apiClient from './api';

const meetingService = {
  /**
   * Start a new meeting
   * @param {Object} meetingData - { meeting_id, title, language }
   * @returns {Promise}
   */
  startMeeting: async (meetingData) => {
    try {
      const response = await apiClient.post('/api/start_meeting', meetingData);
      return response.data;
    } catch (error) {
      throw new Error(error.response?.data?.detail || 'Failed to start meeting');
    }
  },

  /**
   * End a meeting
   * @param {string} meetingId
   * @returns {Promise}
   */
  endMeeting: async (meetingId) => {
    try {
      const response = await apiClient.post('/api/end_meeting', { meeting_id: meetingId });
      return response.data;
    } catch (error) {
      throw new Error(error.response?.data?.detail || 'Failed to end meeting');
    }
  },

  /**
   * Upload audio chunk
   * @param {string} meetingId
   * @param {number} chunkNumber
   * @param {Blob} audioData
   * @param {number} timestamp
   * @param {number} sampleRate
   * @returns {Promise}
   */
  uploadAudioChunk: async (meetingId, chunkNumber, audioData, timestamp, sampleRate = 16000) => {
    try {
      const formData = new FormData();
      formData.append('audio', audioData, `chunk_${chunkNumber}.wav`);

      const response = await apiClient.post('/api/upload_audio', formData, {
        headers: {
          'Content-Type': 'multipart/form-data',
          'X-Meeting-ID': meetingId,
          'X-Chunk-Number': chunkNumber,
          'X-Timestamp': timestamp,
          'X-Sample-Rate': sampleRate,
        },
      });
      return response.data;
    } catch (error) {
      throw new Error(error.response?.data?.detail || 'Failed to upload audio chunk');
    }
  },

  /**
   * Get list of all meetings
   * @returns {Promise<Array>}
   */
  listMeetings: async () => {
    try {
      const response = await apiClient.get('/api/list_meetings');
      return response.data.meetings || [];
    } catch (error) {
      throw new Error(error.response?.data?.detail || 'Failed to fetch meetings');
    }
  },

  /**
   * Get meeting summary
   * @param {string} meetingId
   * @returns {Promise}
   */
  getSummary: async (meetingId) => {
    try {
      const response = await apiClient.get('/api/get_summary', {
        params: { meeting_id: meetingId },
      });
      return response.data;
    } catch (error) {
      throw new Error(error.response?.data?.detail || 'Failed to fetch summary');
    }
  },

  /**
   * Get meeting transcript
   * @param {string} meetingId
   * @returns {Promise}
   */
  getTranscript: async (meetingId) => {
    try {
      const response = await apiClient.get('/api/get_transcript', {
        params: { meeting_id: meetingId },
      });
      return response.data;
    } catch (error) {
      throw new Error(error.response?.data?.detail || 'Failed to fetch transcript');
    }
  },

  /**
   * Get meeting agenda
   * @param {string} meetingId
   * @returns {Promise}
   */
  getAgenda: async (meetingId) => {
    try {
      const response = await apiClient.get('/api/get_agenda', {
        params: { meeting_id: meetingId },
      });
      return response.data;
    } catch (error) {
      throw new Error(error.response?.data?.detail || 'Failed to fetch agenda');
    }
  },
};

export default meetingService;




src/services/qaService.js

import apiClient from './api';

const qaService = {
  /**
   * Ask a question about a meeting
   * @param {string} meetingId
   * @param {string} question
   * @returns {Promise}
   */
  askQuestion: async (meetingId, question) => {
    try {
      const response = await apiClient.post('/api/ask_question', null, {
        params: {
          meeting_id: meetingId,
          question: question,
        },
      });
      return response.data;
    } catch (error) {
      throw new Error(error.response?.data?.detail || 'Failed to get answer');
    }
  },

  /**
   * Get Q&A history for a meeting
   * @param {string} meetingId
   * @returns {Promise<Array>}
   */
  getQAHistory: async (meetingId) => {
    try {
      const response = await apiClient.get('/api/qa_history', {
        params: { meeting_id: meetingId },
      });
      return response.data.history || [];
    } catch (error) {
      throw new Error(error.response?.data?.detail || 'Failed to fetch Q&A history');
    }
  },
};

export default qaService;



src/services/websocketService.js

import { io } from 'socket.io-client';

const WS_BASE_URL = import.meta.env.VITE_WS_BASE_URL || 'ws://localhost:8000';

class WebSocketService {
  constructor() {
    this.socket = null;
    this.listeners = {};
  }

  /**
   * Connect to WebSocket server
   * @param {string} meetingId
   */
  connect(meetingId) {
    if (this.socket && this.socket.connected) {
      console.log('[WebSocket] Already connected');
      return;
    }

    this.socket = io(`${WS_BASE_URL}/ws/meeting/${meetingId}`, {
      transports: ['websocket'],
      reconnection: true,
      reconnectionAttempts: 5,
      reconnectionDelay: 1000,
    });

    this.socket.on('connect', () => {
      console.log(`[WebSocket] Connected to meeting ${meetingId}`);
      this.emit('connection_status', { status: 'connected' });
    });

    this.socket.on('disconnect', (reason) => {
      console.log(`[WebSocket] Disconnected: ${reason}`);
      this.emit('connection_status', { status: 'disconnected', reason });
    });

    this.socket.on('chunk_received', (data) => {
      console.log('[WebSocket] Chunk received:', data);
      this.emit('chunk_received', data);
    });

    this.socket.on('transcription_update', (data) => {
      console.log('[WebSocket] Transcription update:', data);
      this.emit('transcription_update', data);
    });

    this.socket.on('meeting_ended', (data) => {
      console.log('[WebSocket] Meeting ended:', data);
      this.emit('meeting_ended', data);
    });

    this.socket.on('error', (error) => {
      console.error('[WebSocket] Error:', error);
      this.emit('error', error);
    });
  }

  /**
   * Disconnect from WebSocket
   */
  disconnect() {
    if (this.socket) {
      this.socket.disconnect();
      this.socket = null;
      console.log('[WebSocket] Disconnected');
    }
  }

  /**
   * Add event listener
   * @param {string} event
   * @param {Function} callback
   */
  on(event, callback) {
    if (!this.listeners[event]) {
      this.listeners[event] = [];
    }
    this.listeners[event].push(callback);
  }

  /**
   * Remove event listener
   * @param {string} event
   * @param {Function} callback
   */
  off(event, callback) {
    if (this.listeners[event]) {
      this.listeners[event] = this.listeners[event].filter((cb) => cb !== callback);
    }
  }

  /**
   * Emit event to all listeners
   * @param {string} event
   * @param {*} data
   */
  emit(event, data) {
    if (this.listeners[event]) {
      this.listeners[event].forEach((callback) => callback(data));
    }
  }

  /**
   * Send message to server
   * @param {string} event
   * @param {*} data
   */
  send(event, data) {
    if (this.socket && this.socket.connected) {
      this.socket.emit(event, data);
    }
  }
}

export default new WebSocketService();



üóÉÔ∏è Redux Store Setup
src/redux/store.js

import { configureStore } from '@reduxjs/toolkit';
import meetingReducer from './slices/meetingSlice';
import transcriptReducer from './slices/transcriptSlice';
import qaReducer from './slices/qaSlice';
import uiReducer from './slices/uiSlice';
import websocketMiddleware from './middleware/websocketMiddleware';

const store = configureStore({
  reducer: {
    meetings: meetingReducer,
    transcript: transcriptReducer,
    qa: qaReducer,
    ui: uiReducer,
  },
  middleware: (getDefaultMiddleware) =>
    getDefaultMiddleware({
      serializableCheck: {
        // Ignore these action types
        ignoredActions: ['meetings/uploadAudioChunk'],
        // Ignore these paths in the state
        ignoredPaths: ['meetings.audioBuffer'],
      },
    }).concat(websocketMiddleware),
});

export default store;





import { configureStore } from '@reduxjs/toolkit';
import meetingReducer from './slices/meetingSlice';
import transcriptReducer from './slices/transcriptSlice';
import qaReducer from './slices/qaSlice';
import uiReducer from './slices/uiSlice';
import websocketMiddleware from './middleware/websocketMiddleware';

const store = configureStore({
  reducer: {
    meetings: meetingReducer,
    transcript: transcriptReducer,
    qa: qaReducer,
    ui: uiReducer,
  },
  middleware: (getDefaultMiddleware) =>
    getDefaultMiddleware({
      serializableCheck: {
        // Ignore these action types
        ignoredActions: ['meetings/uploadAudioChunk'],
        // Ignore these paths in the state
        ignoredPaths: ['meetings.audioBuffer'],
      },
    }).concat(websocketMiddleware),
});

export default store;



import { createSlice, createAsyncThunk } from '@reduxjs/toolkit';
import meetingService from '@services/meetingService';

// Async thunks
export const startMeeting = createAsyncThunk(
  'meetings/start',
  async (meetingData, { rejectWithValue }) => {
    try {
      return await meetingService.startMeeting(meetingData);
    } catch (error) {
      return rejectWithValue(error.message);
    }
  }
);

export const endMeeting = createAsyncThunk(
  'meetings/end',
  async (meetingId, { rejectWithValue }) => {
    try {
      return await meetingService.endMeeting(meetingId);
    } catch (error) {
      return rejectWithValue(error.message);
    }
  }
);

export const fetchMeetings = createAsyncThunk(
  'meetings/fetchAll',
  async (_, { rejectWithValue }) => {
    try {
      return await meetingService.listMeetings();
    } catch (error) {
      return rejectWithValue(error.message);
    }
  }
);

export const fetchSummary = createAsyncThunk(
  'meetings/fetchSummary',
  async (meetingId, { rejectWithValue }) => {
    try {
      return await meetingService.getSummary(meetingId);
    } catch (error) {
      return rejectWithValue(error.message);
    }
  }
);

export const uploadAudioChunk = createAsyncThunk(
  'meetings/uploadChunk',
  async ({ meetingId, chunkNumber, audioData, timestamp, sampleRate }, { rejectWithValue }) => {
    try {
      return await meetingService.uploadAudioChunk(meetingId, chunkNumber, audioData, timestamp, sampleRate);
    } catch (error) {
      return rejectWithValue(error.message);
    }
  }
);

// Slice
const meetingSlice = createSlice({
  name: 'meetings',
  initialState: {
    meetings: [],
    currentMeeting: null,
    isRecording: false,
    chunkCount: 0,
    summary: null,
    loading: false,
    error: null,
  },
  reducers: {
    setCurrentMeeting: (state, action) => {
      state.currentMeeting = action.payload;
    },
    setRecordingStatus: (state, action) => {
      state.isRecording = action.payload;
    },
    incrementChunkCount: (state) => {
      state.chunkCount += 1;
    },
    resetChunkCount: (state) => {
      state.chunkCount = 0;
    },
    clearError: (state) => {
      state.error = null;
    },
  },
  extraReducers: (builder) => {
    builder
      // Start Meeting
      .addCase(startMeeting.pending, (state) => {
        state.loading = true;
        state.error = null;
      })
      .addCase(startMeeting.fulfilled, (state, action) => {
        state.loading = false;
        state.currentMeeting = action.payload;
        state.isRecording = true;
        state.chunkCount = 0;
      })
      .addCase(startMeeting.rejected, (state, action) => {
        state.loading = false;
        state.error = action.payload;
      })
      // End Meeting
      .addCase(endMeeting.pending, (state) => {
        state.loading = true;
      })
      .addCase(endMeeting.fulfilled, (state, action) => {
        state.loading = false;
        state.isRecording = false;
        if (state.currentMeeting) {
          state.currentMeeting.status = 'completed';
        }
      })
      .addCase(endMeeting.rejected, (state, action) => {
        state.loading = false;
        state.error = action.payload;
      })
      // Fetch Meetings
      .addCase(fetchMeetings.pending, (state) => {
        state.loading = true;
      })
      .addCase(fetchMeetings.fulfilled, (state, action) => {
        state.loading = false;
        state.meetings = action.payload;
      })
      .addCase(fetchMeetings.rejected, (state, action) => {
        state.loading = false;
        state.error = action.payload;
      })
      // Fetch Summary
      .addCase(fetchSummary.pending, (state) => {
        state.loading = true;
      })
      .addCase(fetchSummary.fulfilled, (state, action) => {
        state.loading = false;
        state.summary = action.payload;
      })
      .addCase(fetchSummary.rejected, (state, action) => {
        state.loading = false;
        state.error = action.payload;
      })
      // Upload Chunk
      .addCase(uploadAudioChunk.fulfilled, (state) => {
        state.chunkCount += 1;
      })
      .addCase(uploadAudioChunk.rejected, (state, action) => {
        state.error = action.payload;
      });
  },
});

export const {
  setCurrentMeeting,
  setRecordingStatus,
  incrementChunkCount,
  resetChunkCount,
  clearError,
} = meetingSlice.actions;

export default meetingSlice.reducer;



src/redux/slices/transcriptSlice.js


import { createSlice, createAsyncThunk } from '@reduxjs/toolkit';
import meetingService from '@services/meetingService';

export const fetchTranscript = createAsyncThunk(
  'transcript/fetch',
  async (meetingId, { rejectWithValue }) => {
    try {
      return await meetingService.getTranscript(meetingId);
    } catch (error) {
      return rejectWithValue(error.message);
    }
  }
);

const transcriptSlice = createSlice({
  name: 'transcript',
  initialState: {
    transcript: '',
    segments: [],
    loading: false,
    error: null,
    searchQuery: '',
    filteredSegments: [],
  },
  reducers: {
    setSearchQuery: (state, action) => {
      state.searchQuery = action.payload;
      if (action.payload) {
        state.filteredSegments = state.segments.filter((seg) =>
          seg.text.toLowerCase().includes(action.payload.toLowerCase())
        );
      } else {
        state.filteredSegments = state.segments;
      }
    },
    addTranscriptSegment: (state, action) => {
      state.segments.push(action.payload);
      state.transcript += action.payload.text + ' ';
    },
  },
  extraReducers: (builder) => {
    builder
      .addCase(fetchTranscript.pending, (state) => {
        state.loading = true;
        state.error = null;
      })
      .addCase(fetchTranscript.fulfilled, (state, action) => {
        state.loading = false;
        state.transcript = action.payload.transcript || '';
        state.segments = action.payload.segments || [];
        state.filteredSegments = state.segments;
      })
      .addCase(fetchTranscript.rejected, (state, action) => {
        state.loading = false;
        state.error = action.payload;
      });
  },
});

export const { setSearchQuery, addTranscriptSegment } = transcriptSlice.actions;
export default transcriptSlice.reducer;



src/redux/slices/transcriptSlice.js


import { createSlice, createAsyncThunk } from '@reduxjs/toolkit';
import meetingService from '@services/meetingService';

export const fetchTranscript = createAsyncThunk(
  'transcript/fetch',
  async (meetingId, { rejectWithValue }) => {
    try {
      return await meetingService.getTranscript(meetingId);
    } catch (error) {
      return rejectWithValue(error.message);
    }
  }
);

const transcriptSlice = createSlice({
  name: 'transcript',
  initialState: {
    transcript: '',
    segments: [],
    loading: false,
    error: null,
    searchQuery: '',
    filteredSegments: [],
  },
  reducers: {
    setSearchQuery: (state, action) => {
      state.searchQuery = action.payload;
      if (action.payload) {
        state.filteredSegments = state.segments.filter((seg) =>
          seg.text.toLowerCase().includes(action.payload.toLowerCase())
        );
      } else {
        state.filteredSegments = state.segments;
      }
    },
    addTranscriptSegment: (state, action) => {
      state.segments.push(action.payload);
      state.transcript += action.payload.text + ' ';
    },
  },
  extraReducers: (builder) => {
    builder
      .addCase(fetchTranscript.pending, (state) => {
        state.loading = true;
        state.error = null;
      })
      .addCase(fetchTranscript.fulfilled, (state, action) => {
        state.loading = false;
        state.transcript = action.payload.transcript || '';
        state.segments = action.payload.segments || [];
        state.filteredSegments = state.segments;
      })
      .addCase(fetchTranscript.rejected, (state, action) => {
        state.loading = false;
        state.error = action.payload;
      });
  },
});

export const { setSearchQuery, addTranscriptSegment } = transcriptSlice.actions;
export default transcriptSlice.reducer;




src/redux/slices/qaSlice.js


import { createSlice, createAsyncThunk } from '@reduxjs/toolkit';
import qaService from '@services/qaService';

export const askQuestion = createAsyncThunk(
  'qa/ask',
  async ({ meetingId, question }, { rejectWithValue }) => {
    try {
      return await qaService.askQuestion(meetingId, question);
    } catch (error) {
      return rejectWithValue(error.message);
    }
  }
);

export const fetchQAHistory = createAsyncThunk(
  'qa/fetchHistory',
  async (meetingId, { rejectWithValue }) => {
    try {
      return await qaService.getQAHistory(meetingId);
    } catch (error) {
      return rejectWithValue(error.message);
    }
  }
);

const qaSlice = createSlice({
  name: 'qa',
  initialState: {
    conversations: [],
    loading: false,
    error: null,
  },
  reducers: {
    clearConversations: (state) => {
      state.conversations = [];
    },
  },
  extraReducers: (builder) => {
    builder
      .addCase(askQuestion.pending, (state) => {
        state.loading = true;
        state.error = null;
      })
      .addCase(askQuestion.fulfilled, (state, action) => {
        state.loading = false;
        state.conversations.push({
          question: action.meta.arg.question,
          answer: action.payload.answer,
          timestamp: new Date().toISOString(),
        });
      })
      .addCase(askQuestion.rejected, (state, action) => {
        state.loading = false;
        state.error = action.payload;
      })
      .addCase(fetchQAHistory.fulfilled, (state, action) => {
        state.conversations = action.payload;
      });
  },
});

export const { clearConversations } = qaSlice.actions;
export default qaSlice.reducer;




Key React Components
src/components/meeting/MeetingList.jsx


import React, { useEffect } from 'react';
import { useDispatch, useSelector } from 'react-redux';
import { fetchMeetings } from '@redux/slices/meetingSlice';
import MeetingCard from './MeetingCard';
import LoadingSpinner from '@components/common/LoadingSpinner';
import { Grid, Typography, Box } from '@mui/material';

const MeetingList = () => {
  const dispatch = useDispatch();
  const { meetings, loading, error } = useSelector((state) => state.meetings);

  useEffect(() => {
    dispatch(fetchMeetings());
  }, [dispatch]);

  if (loading) return <LoadingSpinner />;
  if (error) return <Typography color="error">Error: {error}</Typography>;

  return (
    <Box sx={{ padding: 3 }}>
      <Typography variant="h4" gutterBottom>
        Your Meetings
      </Typography>
      <Grid container spacing={3}>
        {meetings.map((meeting) => (
          <Grid item xs={12} sm={6} md={4} key={meeting.meeting_id}>
            <MeetingCard meeting={meeting} />
          </Grid>
        ))}
      </Grid>
    </Box>
  );
};

export default MeetingList;



src/components/meeting/RecordingControls.jsx


import React, { useState } from 'react';
import { useDispatch, useSelector } from 'react-redux';
import { startMeeting, endMeeting } from '@redux/slices/meetingSlice';
import { Button, TextField, Box, CircularProgress } from '@mui/material';
import { Mic, Stop } from '@mui/icons-material';

const RecordingControls = () => {
  const dispatch = useDispatch();
  const { isRecording, currentMeeting, loading } = useSelector((state) => state.meetings);
  const [meetingTitle, setMeetingTitle] = useState('');
  const [language, setLanguage] = useState('en');

  const handleStart = () => {
    const meetingData = {
      meeting_id: `meeting_${Date.now()}`,
      title: meetingTitle || 'Untitled Meeting',
      language: language,
    };
    dispatch(startMeeting(meetingData));
  };

  const handleStop = () => {
    if (currentMeeting) {
      dispatch(endMeeting(currentMeeting.meeting_id));
    }
  };

  return (
    <Box sx={{ display: 'flex', flexDirection: 'column', gap: 2, maxWidth: 400 }}>
      <TextField
        label="Meeting Title"
        value={meetingTitle}
        onChange={(e) => setMeetingTitle(e.target.value)}
        disabled={isRecording}
        fullWidth
      />
      <TextField
        label="Language"
        value={language}
        onChange={(e) => setLanguage(e.target.value)}
        disabled={isRecording}
        fullWidth
        helperText="e.g., en, es, zh, hi"
      />
      {!isRecording ? (
        <Button
          variant="contained"
          color="primary"
          startIcon={loading ? <CircularProgress size={20} /> : <Mic />}
          onClick={handleStart}
          disabled={loading}
          fullWidth
        >
          Start Recording
        </Button>
      ) : (
        <Button
          variant="contained"
          color="error"
          startIcon={<Stop />}
          onClick={handleStop}
          disabled={loading}
          fullWidth
        >
          Stop Recording
        </Button>
      )}
    </Box>
  );
};

export default RecordingControls;


src/components/qa/ChatInterface.jsx

import React, { useState, useEffect } from 'react';
import { useDispatch, useSelector } from 'react-redux';
import { askQuestion, fetchQAHistory } from '@redux/slices/qaSlice';
import {
  Box,
  TextField,
  Button,
  Paper,
  Typography,
  List,
  ListItem,
  CircularProgress,
} from '@mui/material';
import { Send } from '@mui/icons-material';

const ChatInterface = ({ meetingId }) => {
  const dispatch = useDispatch();
  const { conversations, loading } = useSelector((state) => state.qa);
  const [question, setQuestion] = useState('');

  useEffect(() => {
    if (meetingId) {
      dispatch(fetchQAHistory(meetingId));
    }
  }, [meetingId, dispatch]);

  const handleSubmit = (e) => {
    e.preventDefault();
    if (question.trim() && meetingId) {
      dispatch(askQuestion({ meetingId, question }));
      setQuestion('');
    }
  };

  return (
    <Box sx={{ display: 'flex', flexDirection: 'column', height: '100%' }}>
      <Paper elevation={2} sx={{ flexGrow: 1, overflow: 'auto', p: 2, mb: 2 }}>
        <List>
          {conversations.map((conv, idx) => (
            <ListItem key={idx} sx={{ flexDirection: 'column', alignItems: 'flex-start' }}>
              <Typography variant="subtitle1" color="primary" sx={{ fontWeight: 'bold' }}>
                Q: {conv.question}
              </Typography>
              <Typography variant="body1" sx={{ mt: 1 }}>
                A: {conv.answer}
              </Typography>
            </ListItem>
          ))}
        </List>
        {loading && <CircularProgress />}
      </Paper>
      <Box component="form" onSubmit={handleSubmit} sx={{ display: 'flex', gap: 1 }}>
        <TextField
          fullWidth
          variant="outlined"
          placeholder="Ask a question about this meeting..."
          value={question}
          onChange={(e) => setQuestion(e.target.value)}
          disabled={loading}
        />
        <Button
          type="submit"
          variant="contained"
          endIcon={<Send />}
          disabled={loading || !question.trim()}
        >
          Ask
        </Button>
      </Box>
    </Box>
  );
};

export default ChatInterface;



üöÄ Complete Setup & Run Guide
Step 1: Install Prerequisites


# Check Node.js version (requires v16+)
node --version
npm --version

# If not installed, download from https://nodejs.org/



Step 2: Clone & Setup Frontend

# Navigate to project root
cd ~/Projects/VoiceMind-Meeting-System

# Create frontend directory and files (use commands from earlier)
# Then install dependencies
cd voicemind-frontend
npm install



Step 3: Configure Environment

# Copy example env file
cp .env.example .env

# Edit .env with your backend URL
nano .env



Update .env:


VITE_API_BASE_URL=http://localhost:8000
VITE_WS_BASE_URL=ws://localhost:8000


Step 4: Start Backend Server

# In a separate terminal, navigate to backend
cd ~/Projects/VoiceMind-Meeting-System/backend

# Activate virtual environment
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Start FastAPI server
python server.py


Backend should now run on http://localhost:8000
Step 5: Start Frontend Dev Server

# In frontend directory
npm run dev


Frontend should now run on http://localhost:3000

Step 6: Test the System
Option A: Manual Browser Testing

Open browser to http://localhost:3000
Enter a meeting title and click "Start Recording"
Upload audio chunks (simulated or from ESP32)
Click "Stop Recording"
View summary, transcript, and ask questions
Option B: API Testing with cURL



# Test health check
curl http://localhost:8000/

# Start a meeting
curl -X POST "http://localhost:8000/api/start_meeting" \
  -H "Content-Type: application/json" \
  -d '{"meeting_id":"test_001","title":"Test Meeting","language":"en"}'

# List meetings
curl http://localhost:8000/api/list_meetings

# Get summary
curl "http://localhost:8000/api/get_summary?meeting_id=test_001"

# Ask question
curl -X POST "http://localhost:8000/api/ask_question?meeting_id=test_001&question=What%20was%20discussed?"




Build for Production


# Build optimized production bundle
npm run build

# Preview production build locally
npm run preview

# Deploy dist/ folder to your hosting (Vercel, Netlify, AWS S3, etc.)




üìä Full System Integration Test
Test Workflow:
Backend Ready: Ensure FastAPI is running on port 8000
Frontend Ready: Start React dev server on port 3000
ESP32 Ready: Flash firmware and configure WiFi
MySQL Ready: Database schema created and accessible
End-to-End Test Script:


#!/bin/bash
# test_system.sh

echo "=== VoiceMind System Integration Test ==="

# 1. Check backend health
echo "1. Checking backend..."
curl -s http://localhost:8000/ || { echo "Backend not running!"; exit 1; }

# 2. Check frontend
echo "2. Checking frontend..."
curl -s http://localhost:3000/ || { echo "Frontend not running!"; exit 1; }

# 3. Start test meeting
echo "3. Starting test meeting..."
MEETING_ID="test_$(date +%s)"
curl -X POST "http://localhost:8000/api/start_meeting" \
  -H "Content-Type: application/json" \
  -d "{\"meeting_id\":\"$MEETING_ID\",\"title\":\"Integration Test\",\"language\":\"en\"}"

# 4. Simulate audio upload (requires test audio file)
echo "4. Uploading test audio chunk..."
# curl -X POST "http://localhost:8000/api/upload_audio" \
#   -H "X-Meeting-ID: $MEETING_ID" \
#   -H "X-Chunk-Number: 1" \
#   -H "X-Timestamp: $(date +%s)000" \
#   -H "X-Sample-Rate: 16000" \
#   -F "audio=@test_audio.wav"

# 5. End meeting
echo "5. Ending meeting..."
curl -X POST "http://localhost:8000/api/end_meeting" \
  -H "Content-Type: application/json" \
  -d "{\"meeting_id\":\"$MEETING_ID\"}"

# 6. Get summary
echo "6. Fetching summary..."
curl "http://localhost:8000/api/get_summary?meeting_id=$MEETING_ID"

echo ""
echo "=== Test Complete ==="




üêõ Troubleshooting
Common Issues:
1. CORS Errors


// Add to backend server.py
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)



2. WebSocket Connection Failed

Check firewall settings
Verify VITE_WS_BASE_URL in .env
Ensure backend WebSocket endpoint is implemented

3. Audio Upload Fails

Verify Content-Type: multipart/form-data
Check audio file format (WAV, 16kHz, 16-bit)
Increase backend timeout for large files

4. Environment Variables Not Loading

Restart Vite dev server after changing .env
Use VITE_ prefix for all variables
Check import.meta.env.VITE_* syntax

VS Code Debugging:


1. Install extensions:

    ESLint
    Prettier
    Debugger for Chrome

2. Create .vscode/launch.json:

{
  "version": "0.2.0",
  "configurations": [
    {
      "type": "chrome",
      "request": "launch",
      "name": "Launch Chrome",
      "url": "http://localhost:3000",
      "webRoot": "${workspaceFolder}/src"
    }
  ]
}


1. Set breakpoints in React components
2. Press F5 to start debugging


üìù Additional Features to Implement

1. User Authentication (JWT tokens)
2. Real-time Waveform Visualization (Wavesurfer.js)
3. Export Transcript (PDF, DOCX, TXT)
4. Speaker Diarization (Color-coded segments)
5. Analytics Dashboard (Meeting duration, word count, sentiment)
6. Dark Mode Toggle
7. Multi-language UI (i18n)
8. Mobile Responsive Design


üéì Summary
This complete MERN frontend provides:

‚úÖ Full Feature Coverage: Recording controls, meeting list, transcript viewer, Q&A chat, summary display
‚úÖ Redux State Management: Centralized state with async thunks
‚úÖ WebSocket Support: Real-time updates during recording
‚úÖ Material-UI Components: Professional, responsive design
‚úÖ API Integration: All FastAPI endpoints mapped
‚úÖ Production Ready: Build scripts, error handling, logging
‚úÖ VS Code Friendly: Complete folder structure with one-command setup



also add more below to this 

(1). Generate additional components (Analytics Dashboard, Settings Page)
(2). Add specific features (export, speaker diarization, dark mode)



now first analysis,understand hole code,chat carefully please write a code for Frontend Folder Tree hole tree file code provide also connect make a such way that above chat
tree backend connect with frontend later  now do not connect with backend  also draw and write a tree DIAGRAM for Backend Folder Tree in that use mern only but in database i have use mysql
as above chat u can seen in database also for  Backend Folder Tree  write a code for Backend Folder Tree hole tree file code provide help and guide me how to connect frontend and backend both